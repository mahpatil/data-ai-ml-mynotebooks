{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pdfminer.high_level import extract_text as pdf_extract_text\n",
    "import docx2txt\n",
    "import nltk\n",
    "import re\n",
    "import subprocess \n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+', re.IGNORECASE)\n",
    "SKILLS_DB_AWS = [\n",
    "    'aws',\n",
    "    'kubernetes', 'EKS', 'containers', 'docker',\n",
    "    's3',\n",
    "    'rds',\n",
    "    'vpc', 'subnet', \n",
    "    'cloudwatch', 'cloudtrail',\n",
    "    'terraform', 'jenkins'\n",
    " ]\n",
    "SKILLS_DB_AZURE = [\n",
    "    'Azure',\n",
    "    'Kubernetes', 'AKS', 'Containers', 'Docker',\n",
    "    'VNET', 'Firewall', 'AppGateway', 'Application Gateway'\n",
    "    'Azure Monitor', 'Monitor', 'Log Analytics', 'LogAnalytics',\n",
    "    'Terraform', 'Azure DevOps'\n",
    " ]\n",
    "\n",
    "class ResumeProcessor:\n",
    "    text = None\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.text = None\n",
    "        self.phone_numbers = []\n",
    "        self.person_names = []\n",
    "        self.extract_text()\n",
    "        self.extract_names()\n",
    "        self.extract_phone_numbers()\n",
    "        self.extract_emails()\n",
    "        \n",
    "    def extract_text(self):\n",
    "        self.text = None\n",
    "        \n",
    "    def extract_names(self):\n",
    "\n",
    "        for sent in nltk.sent_tokenize(self.text):\n",
    "            for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "                if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                    self.person_names.append(\n",
    "                        ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                    )\n",
    "    def extract_phone_numbers(self):\n",
    "        numbers = re.findall(PHONE_REG, self.text)\n",
    "        for n in numbers:\n",
    "            if len(n.replace(' ', '')) >= 10:\n",
    "                temp_phone = ''.join(n)\n",
    "                if self.text.find(temp_phone) >= 0 and len(temp_phone) < 20:\n",
    "                    print(f'found phone number len>10 and len<20')\n",
    "                    self.phone_numbers.append(temp_phone)\n",
    "    \n",
    "    def extract_emails(self):\n",
    "        self.email_address = re.findall(EMAIL_REG, self.text)\n",
    "\n",
    "\n",
    "    def extract_skills(self, profile_type):\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "        word_tokens = nltk.tokenize.word_tokenize(self.text)\n",
    "\n",
    "        # remove the stop words\n",
    "        filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "        # remove the punctuation\n",
    "        filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "        # generate bigrams and trigrams (such as artificial intelligence)\n",
    "        bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "        # we create a set to keep the results in.\n",
    "        found_skills = set()\n",
    "\n",
    "        if profile_type == 'aws':\n",
    "            SKILLS_DB = SKILLS_DB_AWS\n",
    "        elif profile_type == 'azure':\n",
    "            SKILLS_DB = SKILLS_DB_AZURE\n",
    "\n",
    "        # we search for each token in our skills database\n",
    "        for token in filtered_tokens:\n",
    "            if token.lower() in SKILLS_DB:\n",
    "                found_skills.add(token)\n",
    "\n",
    "        # we search for each bigram and trigram in our skills database\n",
    "        for ngram in bigrams_trigrams:\n",
    "            if ngram.lower() in SKILLS_DB:\n",
    "                found_skills.add(ngram)\n",
    "        #self.profile_type(profile_type)\n",
    "        #self.skills\n",
    "        return found_skills\n",
    "\n",
    "class PdfResumeProcessor(ResumeProcessor):\n",
    "    text = None\n",
    "    def __init__(self, filepath):\n",
    "        super().__init__(filepath)\n",
    "        self.extract_text()\n",
    "\n",
    "    def extract_text(self):\n",
    "        print('**PDF Extract text')\n",
    "        self.text = pdf_extract_text(self.filepath)\n",
    "\n",
    "class DocxResumeProcessor(ResumeProcessor):\n",
    "    def __init__(self, filepath):\n",
    "        super().__init__(filepath)\n",
    "        self.extract_text()\n",
    "    def extract_text(self):\n",
    "        print('**DOCX Extract text')\n",
    "        txt = docx2txt.process(self.filepath)\n",
    "        if txt:\n",
    "            self.text = txt.replace('\\t', ' ')\n",
    "        else:\n",
    "            self.text = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfrp = PdfResumeProcessor('/Users/mahpatil/Downloads/resumes/1.pdf')\n",
    "print(pdfrp.person_names)\n",
    "print(pdfrp.phone_numbers)\n",
    "print(pdfrp.email_address)\n",
    "print(pdfrp.extract_skills('aws'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docrp = DocxResumeProcessor('/Users/mahpatil/Downloads/resumes/2.docx')\n",
    "print(docrp.phone_numbers)\n",
    "print(docrp.email_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}